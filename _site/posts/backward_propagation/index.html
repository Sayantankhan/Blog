<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" data-mode="dark">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations" />
<meta name="author" content="Sayantan" />
<meta property="og:locale" content="en" />
<meta name="description" content="Ever since nonlinear recursive functions (i.e., artificial neural networks) were introduced to the world of machine learning, the range of applications — from chatbots to image generators — has exploded." />
<meta property="og:description" content="Ever since nonlinear recursive functions (i.e., artificial neural networks) were introduced to the world of machine learning, the range of applications — from chatbots to image generators — has exploded." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/posts/backward_propagation/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/posts/backward_propagation/" />
<meta property="og:site_name" content="SayantanKhan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-24T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Sayantan"},"dateModified":"2025-06-24T00:00:00+00:00","datePublished":"2025-06-24T00:00:00+00:00","description":"Ever since nonlinear recursive functions (i.e., artificial neural networks) were introduced to the world of machine learning, the range of applications — from chatbots to image generators — has exploded.","headline":"Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/posts/backward_propagation/"},"url":"http://0.0.0.0:4000/blog/posts/backward_propagation/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations | SayantanKhan
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/blog/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/blog/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/blog/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/blog/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/blog/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SayantanKhan">
<meta name="application-name" content="SayantanKhan">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/blog/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/blog/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/blog/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script>







<script defer src="/blog/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script src="/blog/assets/js/data/mathjax.js"></script>
  <script async src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>


<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/blog/" id="avatar" class="rounded-circle"><img src="/blog/assets/img/avatar.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/blog/">SayantanKhan</a>
    <p class="site-subtitle fst-italic mb-0">Blog Posts</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/blog/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/blog/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    

    
      

      
        <a
          href="https://github.com/Sayantankhan"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['iamstk14','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/blog/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/blog/">Home</a>
            </span>

          
        
          
        
          
            
              <span>Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->





<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations</h1>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1750723200"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Jun 24, 2025
</time>

      </span>

      <!-- lastmod date -->
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://www.linkedin.com/in/sayantan-khan-219231b7">Sayantan</a>
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="1970 words"
>
  <em>10 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <p>Ever since nonlinear recursive functions (i.e., artificial neural networks) were introduced to the world of machine learning, the range of applications — from chatbots to image generators — has exploded.</p>

<p>But behind every smart AI model is a training process. And at the heart of that training lies one magic trick:</p>

<blockquote>
  <p><strong>Backpropagation.</strong></p>
</blockquote>

<p>Despite being fundamental, it remains one of the most intimidating topics for newcomers.</p>

<hr />

<h2 id="-what-is-backpropagation"><span class="me-2">🧠 What <em>Is</em> Backpropagation?</span><a href="#-what-is-backpropagation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Backpropagation (short for <em>backward propagation of errors</em>) is the process by which a neural network learns from its mistakes.</p>

<p>Think of it this way:</p>

<ol>
  <li>You ask the model a question.</li>
  <li>It gives you an answer.</li>
  <li>You check how wrong it was.</li>
  <li>You go backward through the network and adjust all the knobs (weights) so it can do better next time.</li>
</ol>

<p>This “adjustment” is what backpropagation does using <strong>calculus</strong> — particularly the <strong>chain rule</strong>.</p>

<p>Training neural networks requires adjusting weights to minimize the error between predicted and true labels. This is done using:</p>

<ul>
  <li>🔗 <strong>Chain Rule</strong> from calculus</li>
  <li>🔥 <strong>Softmax Activation</strong> for multiclass classification</li>
  <li>📉 <strong>Cross-Entropy Loss</strong> to measure prediction error</li>
  <li>🔄 <strong>Backpropagation</strong> to update weights</li>
</ul>

<p>This post walks through the math behind these concepts — step by step.</p>

<hr />

<h2 id="-lets-jump-to-the-basics"><span class="me-2">🚀 Let’s Jump to the Basics</span><a href="#-lets-jump-to-the-basics" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Before diving into math, let’s walk through a full forward-and-backward cycle with a tiny neural network.</p>

<p>We’ll show:</p>
<ul>
  <li>How the model makes a prediction</li>
  <li>How we measure its mistake</li>
  <li>How it updates itself using gradients</li>
</ul>

<h3 id="-1-softmax-activation-function"><span class="me-2">🧠 1. Softmax Activation Function</span><a href="#-1-softmax-activation-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Given logits $z = [z_1, z_2, \dots, z_K]$ from the last layer of the model, the softmax function converts them to probabilities:</p>

\[\hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}\]

<p>Properties:</p>
<ul>
  <li>$\hat{y}_i \in (0, 1)$</li>
  <li>$\sum_{i=1}^K \hat{y}_i = 1$</li>
</ul>

<h3 id="️-2-one-hot-encoding--cross-entropy-loss"><span class="me-2">🏷️ 2. One-Hot Encoding &amp; Cross-Entropy Loss</span><a href="#️-2-one-hot-encoding--cross-entropy-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Assume the true label is one-hot encoded:</p>

<ul>
  <li>For class $c$, $y = [0, 0, \dots, 1, \dots, 0]$ where only $y_c = 1$</li>
</ul>

<p>Then the cross-entropy loss is:</p>

\[L = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)\]

<p>Since only $y_c = 1$, this simplifies to:</p>

\[L = -\log(\hat{y}_c)\]

<p><strong>Example</strong>:<br />
If predicted output is $\hat{y} = [0.1, 0.2, 0.6, 0.1]$ and the correct class is index 3:</p>

\[L = -\log(0.6) \approx 0.51\]

<h3 id="-3-backpropagation-using-the-chain-rule"><span class="me-2">🔄 3. Backpropagation: Using the Chain Rule</span><a href="#-3-backpropagation-using-the-chain-rule" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>We aim to compute the gradient:</p>

\[\frac{dL}{dx} = \frac{dL}{d\hat{y}} \cdot \frac{d\hat{y}}{dz} \cdot \frac{dz}{dx}\]

<p>Let:</p>
<ul>
  <li>$z_i = \text{logit for class } i$</li>
  <li>$\hat{y}<em>i = \frac{e^{z_i}}{\sum</em>{j=1}^n e^{z_j}}$ — softmax output</li>
  <li>$L = -\log(\hat{y}_c)$ — cross-entropy loss for class $c$</li>
  <li>$x \in \mathbb{R}^d$, input vector</li>
</ul>

<h2 id="-the-math-behind-it-all-gradients-step-by-step"><span class="me-2">🧮 The Math Behind It All: Gradients Step-by-Step</span><a href="#-the-math-behind-it-all-gradients-step-by-step" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>At this point, we’ve seen <em>what</em> backpropagation does and the functions involved for the process</p>

<p>Now let’s see <em>how</em> it does that — by computing gradients layer by layer using calculus. Specifically, we’ll break down how the <strong>cross-entropy loss</strong> and <strong>softmax</strong> work together during backpropagation.</p>

<p>This section walks through:</p>
<ul>
  <li>How to compute the gradient of the loss with respect to the softmax output</li>
  <li>How the softmax output changes with logits</li>
  <li>And how everything connects using the <strong>chain rule</strong></li>
</ul>

<p>These formulas form the backbone of how your model “learns” to push probabilities in the right direction.</p>

<hr />

<h3 id="step-1️⃣--derivative-of-loss-wrt-softmax-output"><span class="me-2">Step 1️⃣ — Derivative of Loss w.r.t. Softmax Output</span><a href="#step-1️⃣--derivative-of-loss-wrt-softmax-output" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>We start by calculating how the loss changes with respect to each predicted probability $\hat{y}_i$ from the softmax layer.</p>

<p>We will begin with the <strong>cross-entropy loss</strong> for a single example, where the correct class is $c$:</p>

\[L = -\log(\hat{y}_c)\]

<p>Now, recall that softmax converts logits $z_k$ into predicted probabilities $\hat{y}_k$:</p>

\[\hat{y}_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}\]

<p>Substituting the softmax expression for $\hat{y}_c$ into the loss:</p>

\[L = -\log\left(\frac{e^{z_c}}{\sum_{j=1}^{K} e^{z_j}}\right)\]

<p>Apply the log rule:</p>

\[L = -z_c + \log\left(\sum_{j=1}^{K} e^{z_j}\right)\]

<p>Now we differentiate the loss $L$ with respect to each logit $z_k$.</p>

<p>We do this by taking the derivative:</p>

\[\frac{dL}{dz_k} = \frac{d}{dz_k} \left[ -z_c + \log\left(\sum_{j=1}^{K} e^{z_j} \right) \right]\]

<p>There are two cases to consider:</p>

<hr />

<h4 id="case-1-k--c--we-are-computing-the-gradient-with-respect-to-the-logit-corresponding-to-the-correct-class"><span class="me-2">Case 1: $k = c$ : We are computing the gradient with respect to the logit corresponding to the correct class.</span><a href="#case-1-k--c--we-are-computing-the-gradient-with-respect-to-the-logit-corresponding-to-the-correct-class" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>First term derivative: $\frac{d(-z_c)}{dz_c} = -1$</li>
  <li>
    <p>Second term derivative:</p>

\[\frac{d}{dz_c} \log\left(\sum_{j=1}^{K} e^{z_j}\right) = \frac{e^{z_c}}{\sum_{j=1}^{K} e^{z_j}} = \hat{y}_c\]
  </li>
</ul>

<p>So:</p>

\[\frac{dL}{dz_c} = -1 + \hat{y}_c = \hat{y}_c - 1\]

<hr />

<h4 id="case-2-k-ne-c--we-are-computing-the-gradient-with-respect-to-the-logit-corresponding-to-a-class-other-than-the-correct-one"><span class="me-2">Case 2: $k \ne c$ : We are computing the gradient with respect to the logit corresponding to a class other than the correct one.</span><a href="#case-2-k-ne-c--we-are-computing-the-gradient-with-respect-to-the-logit-corresponding-to-a-class-other-than-the-correct-one" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>First term derivative: 0</li>
  <li>
    <p>Second term derivative:</p>

\[\frac{d}{dz_k} \log\left(\sum_{j=1}^{K} e^{z_j}\right) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} = \hat{y}_k\]
  </li>
</ul>

<p>So:</p>

\[\frac{dL}{dz_k} = \hat{y}_k\]

<h3 id="-why-this-matters-case-2-k-ne-c"><span class="me-2">🚨 Why This Matters (Case 2: $k \ne c$)</span><a href="#-why-this-matters-case-2-k-ne-c" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>When your model assigns <strong>high probability to a wrong class</strong>, it’s not just incorrect — it’s <em>confidently wrong</em>. That’s what the gradient helps fix.</p>

<p>We compute the gradient of the loss with respect to the logit $z_k$ for an incorrect class ($k \ne c$):</p>

\[\frac{dL}{dz_k} = \hat{y}_k\]

<p>This value is always <strong>positive</strong>, since $\hat{y}_k \in (0, 1)$.</p>

<h4 id="-what-this-tells-us"><span class="me-2">🔎 What This Tells Us:</span><a href="#-what-this-tells-us" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>
    <p>If $\hat{y}_k$ is <strong>large</strong>, the model is confidently wrong.<br />
→ The gradient is large<br />
→ <strong>Push the logit $z_k$ down</strong> to reduce its influence</p>
  </li>
  <li>
    <p>If $\hat{y}_k$ is <strong>small</strong>, the model is unsure about class $k$.<br />
→ The gradient is small<br />
→ <strong>Don’t change $z_k$ much</strong></p>
  </li>
</ul>

<hr />

<h3 id="-interpreting-positive-vs-negative-gradients"><span class="me-2">🔁 Interpreting Positive vs Negative Gradients</span><a href="#-interpreting-positive-vs-negative-gradients" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Think of gradients as instructions for moving toward lower loss:</p>

<ul>
  <li>
    <p><strong>Positive gradient</strong>:<br />
$\frac{dL}{dz_k} &gt; 0$<br />
→ Increasing $z_k$ increases loss<br />
→ So, we should <strong>decrease $z_k$</strong></p>
  </li>
  <li>
    <p><strong>Negative gradient</strong>:<br />
$\frac{dL}{dz_k} &lt; 0$<br />
→ Increasing $z_k$ decreases loss<br />
→ So, we should <strong>increase $z_k$</strong></p>
  </li>
</ul>

<hr />

<h3 id="-intuition"><span class="me-2">🧠 Intuition:</span><a href="#-intuition" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th>Class Type</th>
      <th>Gradient</th>
      <th>Action During Training</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Correct ($k = c$)</td>
      <td>$\hat{y}_c - 1$ (negative if $\hat{y}_c &lt; 1$)</td>
      <td>Boost this logit’s value to improve prediction</td>
    </tr>
    <tr>
      <td>Incorrect ($k \ne c$)</td>
      <td>$\hat{y}_k$ (always positive)</td>
      <td>Lower this logit’s value to reduce confusion</td>
    </tr>
  </tbody>
</table></div>

<p>This elegant gradient behavior is why softmax + cross-entropy is the go-to combo for classification problems in deep learning.</p>

<h3 id="-final-combined-result"><span class="me-2">✅ Final Combined Result:</span><a href="#-final-combined-result" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>We can now write this as a single equation for all $k$:</p>

\[\frac{dL}{dz_k} = \hat{y}_k - y_k\]

<p>Where $y_k = 1$ if $k = c$, else $0$ (i.e., one-hot encoded target).</p>

<p>This result is <strong>elegant, efficient</strong>, and used widely in deep learning frameworks for training classification models.</p>

<hr />

<h3 id="step-2️⃣--derivative-of-softmax-output-wrt-logit"><span class="me-2">Step 2️⃣ — Derivative of Softmax Output w.r.t. Logit</span><a href="#step-2️⃣--derivative-of-softmax-output-wrt-logit" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Next, we ask: <em>how does changing a logit $z_k$ affect the predicted probabilities $\hat{y}_i$?</em></p>

<p>This gives us the Jacobian of softmax:</p>

\[\frac{\partial \hat{y}_i}{\partial z_k} =
\begin{cases}
\hat{y}_i(1 - \hat{y}_i) &amp; \text{if } i = k \\
-\hat{y}_i \hat{y}_k &amp; \text{if } i \ne k
\end{cases}\]

<p>This captures the interaction between logits — softmax isn’t independent across dimensions!</p>

<hr />

<h3 id="step-3️⃣--combine-using-chain-rule"><span class="me-2">Step 3️⃣ — Combine Using Chain Rule</span><a href="#step-3️⃣--combine-using-chain-rule" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Now, we apply the <strong>chain rule</strong> to connect the loss to the logits:</p>

\[\frac{dL}{dz_k} = \sum_i \frac{dL}{d\hat{y}_i} \cdot \frac{d\hat{y}_i}{dz_k}\]

<p>Let’s compute both cases:</p>

<h4 id="case-1-k--c-correct-class"><span class="me-2">Case 1: $k = c$ (correct class)</span><a href="#case-1-k--c-correct-class" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\frac{dL}{dz_c} = -\frac{1}{\hat{y}_c} \cdot \hat{y}_c (1 - \hat{y}_c) = \hat{y}_c - 1\]

<h4 id="case-2-k-ne-c"><span class="me-2">Case 2: $k \ne c$</span><a href="#case-2-k-ne-c" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

\[\frac{dL}{dz_k} = -\frac{1}{\hat{y}_c} \cdot (-\hat{y}_c \hat{y}_k) = \hat{y}_k\]

<hr />

<h3 id="-final-result"><span class="me-2">✅ Final Result:</span><a href="#-final-result" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>The gradient of the loss with respect to each logit $z_k$ is simply:</p>

\[\frac{dL}{dz_k} = \hat{y}_k - y_k\]

<p>This <strong>elegant result</strong> is why the softmax + cross-entropy combo is so popular — the gradient simplifies beautifully!</p>

<hr />

<h2 id="-4-derivative-wrt-input-fracdldx"><span class="me-2">🧾 4. Derivative w.r.t. Input: $\frac{dL}{dx}$</span><a href="#-4-derivative-wrt-input-fracdldx" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Lastly, we connect the gradient all the way back to the input layer.</p>

<p>If each logit $z_k$ was computed as:</p>

\[z_k = \mathbf{w}_k^\top \mathbf{x} + b_k\]

<p>Then:</p>

\[\frac{dz_k}{dx} = \mathbf{w}_k\]

<p>So the full gradient of the loss with respect to the input vector $x$ is:</p>

\[\frac{dL}{dx} = \sum_{k=1}^{K} \frac{dL}{dz_k} \cdot \frac{dz_k}{dx} = \sum_{k=1}^{K} (\hat{y}_k - y_k) \cdot \mathbf{w}_k\]

<p>Or, more compactly in matrix form:</p>

\[\boxed{
\frac{dL}{dx} = W^\top (\hat{\mathbf{y}} - \mathbf{y})
}\]

<p>Where:</p>
<ul>
  <li>$W \in \mathbb{R}^{K \times d}$ is the weight matrix from input to logits</li>
  <li>$(\hat{y} - y)$ is the prediction error vector</li>
</ul>

<hr />

<h2 id="-summary"><span class="me-2">📌 Summary</span><a href="#-summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Formula</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Softmax</td>
      <td>$\hat{y}_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$</td>
    </tr>
    <tr>
      <td>Cross-Entropy Loss</td>
      <td>$L = -\log(\hat{y}_{\text{true}})$</td>
    </tr>
    <tr>
      <td>Gradient w.r.t. $z_k$</td>
      <td>$\frac{dL}{dz_k} = \hat{y}_k - y_k$</td>
    </tr>
    <tr>
      <td>Gradient w.r.t. $x$</td>
      <td>$\frac{dL}{dx} = W^\top (\hat{\mathbf{y}} - \mathbf{y})$</td>
    </tr>
  </tbody>
</table></div>

<hr />

<h2 id="-intuition-1"><span class="me-2">💡 Intuition</span><a href="#-intuition-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<ul>
  <li>Softmax transforms logits to probabilities</li>
  <li>Cross-entropy penalizes confident wrong predictions</li>
  <li>The chain rule enables gradient flow through all layers</li>
  <li>The combined gradient simplifies beautifully to:  \(\boxed{\hat{y}_k - y_k}\)</li>
</ul>

<p>This efficient gradient powers the entire backpropagation algorithm in modern deep learning frameworks.</p>

<hr />
<h2 id="-example-the-capital-of-india-is"><span class="me-2">🧪 Example: “The Capital of India Is…”</span><a href="#-example-the-capital-of-india-is" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Let’s solidify everything with a concrete example.</p>

<p>Suppose we’re training a neural network to answer the question:</p>

<blockquote>
  <p>❓ <strong>“The capital of India is…”</strong></p>
</blockquote>

<p>The output layer has 3 neurons (classes):</p>

<ol>
  <li>🏙️ Delhi</li>
  <li>🌆 Bangalore</li>
  <li>🌇 Mumbai</li>
</ol>

<p>Let’s say the final layer outputs raw scores (logits):</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>z = [2.0, 1.0, 0.1]
</pre></td></tr></tbody></table></code></div></div>

<p>We apply the <strong>softmax</strong>:</p>

\[\hat{y}_i = \frac{e^{z_i}}{\sum_j e^{z_j}}\]

<p>Softmax converts $z$ into probabilities:</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>softmax(z) = [0.659, 0.242, 0.099]
</pre></td></tr></tbody></table></code></div></div>

<p>So the model is <strong>most confident</strong> that the answer is <strong>Delhi</strong>.</p>

<p>Let’s assume:</p>
<ul>
  <li>The <strong>correct answer is Delhi</strong> (index 0)</li>
  <li>The <strong>true label</strong> (one-hot encoded): <code class="language-plaintext highlighter-rouge">[1, 0, 0]</code></li>
</ul>

<hr />

<h3 id="-step-1-cross-entropy-loss"><span class="me-2">🧮 Step 1: Cross-Entropy Loss</span><a href="#-step-1-cross-entropy-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

\[L = -\log(\hat{y}_{\text{true}}) = -\log(0.659) \approx 0.417\]

<hr />

<h3 id="-step-2-gradients-wrt-logits"><span class="me-2">🧮 Step 2: Gradients w.r.t. Logits</span><a href="#-step-2-gradients-wrt-logits" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>We apply:</p>

\[\frac{dL}{dz_k} = \hat{y}_k - y_k\]

<p>Compute for each class:</p>

<ul>
  <li>
    <p>$k = 0$ (Delhi, correct):<br />
$\hat{y}_0 - y_0 = 0.659 - 1 = -0.341$</p>
  </li>
  <li>
    <p>$k = 1$ (Bangalore):<br />
$\hat{y}_1 - y_1 = 0.242 - 0 = 0.242$</p>
  </li>
  <li>
    <p>$k = 2$ (Mumbai):<br />
$\hat{y}_2 - y_2 = 0.099 - 0 = 0.099$</p>
  </li>
</ul>

<hr />

<h3 id="-what-this-means"><span class="me-2">💡 What This Means</span><a href="#-what-this-means" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ul>
  <li>
    <p>Negative gradient for <strong>Delhi</strong>:<br />
→ Increase its logit to become even more confident</p>
  </li>
  <li>
    <p>Positive gradients for <strong>Bangalore</strong> and <strong>Mumbai</strong>:<br />
→ Decrease their logits to reduce confusion</p>
  </li>
</ul>

<hr />

<h3 id="-final-backprop-step-input-gradient"><span class="me-2">🔄 Final Backprop Step: Input Gradient</span><a href="#-final-backprop-step-input-gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Assume weights:</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>W = [
  [0.2, 0.5],    # weights for Delhi
  [-0.3, 0.8],   # weights for Bangalore
  [0.1, -0.6]    # weights for Mumbai
]
</pre></td></tr></tbody></table></code></div></div>

<p>Then:</p>

\[\frac{dL}{dx} = W^\top (\hat{y} - y)\]

<p>Compute:</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>dz = [ -0.341, 0.242, 0.099 ]

Wᵗ @ dz =
= [
    (0.2)(-0.341) + (-0.3)(0.242) + (0.1)(0.099),
    (0.5)(-0.341) + (0.8)(0.242) + (-0.6)(0.099)
  ]
= [ -0.068, -0.014 ]
</pre></td></tr></tbody></table></code></div></div>

<p>This tells us how to nudge the <strong>input layer</strong> to improve the prediction.</p>

<hr />

<h2 id="-pytorch-code-softmax--cross-entropy--manual-backward"><span class="me-2">✅ PyTorch Code: Softmax + Cross-Entropy + Manual Backward</span><a href="#-pytorch-code-softmax--cross-entropy--manual-backward" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># 1. Inputs and weights (from blog example)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Input vector
</span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>    <span class="c1"># Delhi
</span>    <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>   <span class="c1"># Bangalore
</span>    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">]</span>    <span class="c1"># Mumbai
</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Bias
</span>
<span class="c1"># 2. Logits: z = Wx + b
</span><span class="n">z</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># shape: [3]
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Logits (z):</span><span class="sh">"</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

<span class="c1"># 3. Softmax
</span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted probabilities (softmax):</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="nf">round</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">y_hat</span><span class="p">])</span>

<span class="c1"># 4. True label (class index) – Delhi
</span><span class="n">y_true_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 5. Cross-entropy loss manually
</span><span class="n">loss_manual</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="n">y_true_idx</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Manual cross-entropy loss:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">loss_manual</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># 6. Built-in CrossEntropyLoss
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss_builtin</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">y_true_idx</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Built-in CrossEntropyLoss:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">loss_builtin</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># 7. Manual gradient: dL/dz = y_hat - y_true
</span><span class="n">y_true_one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="n">y_true_one_hot</span><span class="p">[</span><span class="n">y_true_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">dz</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y_true_one_hot</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Manual dL/dz:</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="nf">round</span><span class="p">(</span><span class="n">d</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dz</span><span class="p">])</span>

<span class="c1"># 8. Backprop to input: dL/dx = W.T @ dz
</span><span class="n">dx</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Manual dL/dx:</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="nf">round</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dx</span><span class="p">])</span>

<span class="c1"># 9. Autograd verification
</span><span class="n">loss_builtin</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Autograd dL/dx:</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="nf">round</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">])</span>

</pre></td></tr></tbody></table></code></div></div>

<hr />
<h2 id="-conclusion"><span class="me-2">🧠 Conclusion</span><a href="#-conclusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Backpropagation is the heart of how neural networks learn.</p>

<p>In this post, we:</p>

<ul>
  <li>Introduced <strong>Softmax</strong> to turn raw scores into probabilities</li>
  <li>Explained how <strong>Cross-Entropy Loss</strong> penalizes wrong predictions</li>
  <li>Used the <strong>Chain Rule</strong> to compute gradients step-by-step</li>
  <li>Showed how the final update boils down to this elegant result:</li>
</ul>

\[\frac{dL}{dz_k} = \hat{y}_k - y_k\]

<ul>
  <li>Walked through a real-world example (answering “The capital of India is…”)</li>
  <li>Verified it all in <strong>PyTorch</strong> — both manually and with autograd</li>
</ul>

<p>This gradient — simple yet powerful — is what enables neural nets to tweak weights, minimize error, and eventually make smarter predictions.</p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/blog/tags/neural-networks/"
            class="post-tag no-text-decoration"
          >neural-networks</a>
        
          <a
            href="/blog/tags/deep-learning/"
            class="post-tag no-text-decoration"
          >deep-learning</a>
        
          <a
            href="/blog/tags/backpropagation/"
            class="post-tag no-text-decoration"
          >backpropagation</a>
        
          <a
            href="/blog/tags/softmax/"
            class="post-tag no-text-decoration"
          >softmax</a>
        
          <a
            href="/blog/tags/cross-entropy/"
            class="post-tag no-text-decoration"
          >cross-entropy</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Backpropagation,%20Softmax,%20Cross-Entropy,%20and%20the%20Chain%20Rule%20%E2%80%94%20Explained%20with%20Equations%20-%20SayantanKhan&url=http%3A%2F%2F0.0.0.0%3A4000%2Fblog%2Fposts%2Fbackward_propagation%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Backpropagation,%20Softmax,%20Cross-Entropy,%20and%20the%20Chain%20Rule%20%E2%80%94%20Explained%20with%20Equations%20-%20SayantanKhan&u=http%3A%2F%2F0.0.0.0%3A4000%2Fblog%2Fposts%2Fbackward_propagation%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2F0.0.0.0%3A4000%2Fblog%2Fposts%2Fbackward_propagation%2F&text=Backpropagation,%20Softmax,%20Cross-Entropy,%20and%20the%20Chain%20Rule%20%E2%80%94%20Explained%20with%20Equations%20-%20SayantanKhan" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/backward_propagation/">Backpropagation, Softmax, Cross-Entropy, and the Chain Rule — Explained with Equations</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/tensorflow_grpc/">🚀 Accelerating TensorFlow with Adaptive RDMA ⚡️: Supercharging gRPC Performance 🔗</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/firecracker-vm/">🔥 Firecracker: The Lightweight Guard of Serverless Computing</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/flexi-raft/">FlexiRaft: Flexible Quorums with Raft in MySQL</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/ha-cluster/">High Availability Demo 🚀 Pacemaker + Corosync + Docker</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/aws/">aws</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/backpropagation/">backpropagation</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/clustering/">clustering</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/consensus/">consensus</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/corosync/">corosync</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/cross-entropy/">cross-entropy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/distributed-systems/">distributed-systems</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/docker/">docker</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/flexiquorum/">flexiquorum</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/blog/posts/tensorflow_grpc/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>🚀 Accelerating TensorFlow with Adaptive RDMA ⚡️: Supercharging gRPC Performance 🔗</p>
    </a>
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Newer">
      <p>-</p>
    </div>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2025</time>

    
      <a href="https://www.linkedin.com/in/sayantan-khan-219231b7">Sayantan</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.3.0"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/aws/">aws</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/backpropagation/">backpropagation</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/clustering/">clustering</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/consensus/">consensus</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/corosync/">corosync</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/cross-entropy/">cross-entropy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/distributed-systems/">distributed-systems</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/docker/">docker</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/flexiquorum/">flexiquorum</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/blog/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

