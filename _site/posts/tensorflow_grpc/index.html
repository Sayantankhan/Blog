<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" data-mode="dark">
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó" />
<meta name="author" content="Sayantan" />
<meta property="og:locale" content="en" />
<meta name="description" content="üß† This blog post is evolved from the key insights presented in the paper Characterizing and Optimizing Distributed Deep Learning Training on Modern GPU Clusters. The original work deeply analyzes the communication bottlenecks in Distributed TensorFlow and motivates the need for more adaptive and efficient communication strategies." />
<meta property="og:description" content="üß† This blog post is evolved from the key insights presented in the paper Characterizing and Optimizing Distributed Deep Learning Training on Modern GPU Clusters. The original work deeply analyzes the communication bottlenecks in Distributed TensorFlow and motivates the need for more adaptive and efficient communication strategies." />
<link rel="canonical" href="http://0.0.0.0:4000/blog/posts/tensorflow_grpc/" />
<meta property="og:url" content="http://0.0.0.0:4000/blog/posts/tensorflow_grpc/" />
<meta property="og:site_name" content="SayantanKhan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-23T04:30:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Sayantan"},"dateModified":"2025-06-23T06:38:29+00:00","datePublished":"2025-06-23T04:30:00+00:00","description":"üß† This blog post is evolved from the key insights presented in the paper Characterizing and Optimizing Distributed Deep Learning Training on Modern GPU Clusters. The original work deeply analyzes the communication bottlenecks in Distributed TensorFlow and motivates the need for more adaptive and efficient communication strategies.","headline":"üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/blog/posts/tensorflow_grpc/"},"url":"http://0.0.0.0:4000/blog/posts/tensorflow_grpc/"}</script>
<!-- End Jekyll SEO tag -->


  <title>üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó | SayantanKhan
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/blog/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/blog/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/blog/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/blog/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/blog/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="SayantanKhan">
<meta name="application-name" content="SayantanKhan">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/blog/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/blog/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/blog/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script>







<script defer src="/blog/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/blog/" id="avatar" class="rounded-circle"><img src="/blog/assets/img/avatar.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/blog/">SayantanKhan</a>
    <p class="site-subtitle fst-italic mb-0">Blog Posts</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/blog/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/blog/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/blog/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    

    
      

      
        <a
          href="https://github.com/Sayantankhan"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['iamstk14','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/blog/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/blog/">Home</a>
            </span>

          
        
          
        
          
            
              <span>üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó</h1>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1750653000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Jun 23, 2025
</time>

      </span>

      <!-- lastmod date -->
      
        <span>
          Updated
          <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1750660709"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Jun 23, 2025
</time>

        </span>
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://www.linkedin.com/in/sayantan-khan-219231b7">Sayantan</a>
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="3265 words"
>
  <em>18 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <blockquote>
  <h2 id="-this-blog-post-is-evolved-from-the-key-insights-presented-in-the-paper-characterizing-and-optimizing-distributed-deep-learning-training-on-modern-gpu-clusters-the-original-work-deeply-analyzes-the-communication-bottlenecks-in-distributed-tensorflow-and-motivates-the-need-for-more-adaptive-and-efficient-communication-strategies"><span class="me-2">üß† <em>This blog post is evolved from the key insights presented in the paper <a href="https://par.nsf.gov/servlets/purl/10112767">Characterizing and Optimizing Distributed Deep Learning Training on Modern GPU Clusters</a>. The original work deeply analyzes the communication bottlenecks in Distributed TensorFlow and motivates the need for more adaptive and efficient communication strategies.</em></span><a href="#-this-blog-post-is-evolved-from-the-key-insights-presented-in-the-paper-characterizing-and-optimizing-distributed-deep-learning-training-on-modern-gpu-clusters-the-original-work-deeply-analyzes-the-communication-bottlenecks-in-distributed-tensorflow-and-motivates-the-need-for-more-adaptive-and-efficient-communication-strategies" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>
</blockquote>

<p>TensorFlow is one of the most widely used deep learning frameworks, powering everything from research experiments to large-scale production systems. As models grow in size and complexity, distributing training across multiple nodes becomes essential ‚Äî and communication overhead quickly becomes a bottleneck.</p>

<p>To tackle this, <strong>Distributed TensorFlow</strong> supports multiple transport channels designed to efficiently transfer tensors across machines, each with different performance characteristics:</p>

<h2 id="-tensor-communication-options-in-tensorflow-distributed"><span class="me-2">üì° Tensor Communication Options in TensorFlow Distributed</span><a href="#-tensor-communication-options-in-tensorflow-distributed" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<ul>
  <li><strong>gRPC over TCP/IP</strong> ‚Äì The default communication channel using standard networking.</li>
  <li><strong>gRPC over IP over InfiniBand (IB)</strong> ‚Äì Utilizes IB hardware but still incurs TCP/IP overhead.</li>
  <li><strong>gRPC + MPI</strong> ‚Äì Leverages MPI libraries for efficient, collective communication.</li>
  <li><strong>gRPC + Verbs</strong> ‚Äì Uses RDMA verbs directly for low-latency, zero-copy data movement.</li>
  <li><strong>AR-gRPC (Adaptive RDMA-based gRPC)</strong> ‚Äì An intelligent layer that dynamically adapts the communication path based on runtime conditions for optimal performance.</li>
</ul>

<hr />
<h3 id="-channel-vs-mechanism-comparison"><span class="me-2">üìä Channel vs Mechanism Comparison</span><a href="#-channel-vs-mechanism-comparison" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th><strong>Channel</strong></th>
      <th><strong>Main Mechanism</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gRPC</td>
      <td>TCP/IP, IP-over-IB</td>
    </tr>
    <tr>
      <td>gRPC + Verbs</td>
      <td>RDMA for Tensor transfers; gRPC for Others</td>
    </tr>
    <tr>
      <td>gRPC + MPI</td>
      <td>MPI for Tensor transfers</td>
    </tr>
    <tr>
      <td>AR-gRPC</td>
      <td>Native RDMA; Adaptive Communication for Tensor Demands</td>
    </tr>
  </tbody>
</table></div>

<hr />

<h2 id="-motivation"><span class="me-2">üí° Motivation</span><a href="#-motivation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>High-performance interconnects and high-speed communication protocols like <strong>RDMA (Remote Direct Memory Access)</strong> have revolutionized data movement across distributed systems. By bypassing the kernel and avoiding unnecessary memory copies, RDMA provides <strong>low-latency, high-throughput</strong> communication‚Äîmaking it ideal for large-scale data-intensive workloads.</p>

<p>Many system-level frameworks, such as:</p>
<ul>
  <li>Apache Hadoop,</li>
  <li>Apache Spark,</li>
  <li>Key-Value Stores, and</li>
  <li>Deep Learning frameworks (like TensorFlow),</li>
</ul>

<p>can significantly benefit from integrating <strong>native RDMA support</strong>.</p>

<hr />

<h2 id="-key-questions"><span class="me-2">‚ùì Key Questions</span><a href="#-key-questions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>With these advancements, we are prompted to explore:</p>

<ol>
  <li>
    <p><strong>Can these new RDMA-based channels actually improve DL (Deep Learning) workload performance?</strong></p>
  </li>
  <li>
    <p><strong>Among the existing RDMA-enabled channels‚ÄîVerbs, MPI, IP-over-IB, AR-gRPC‚Äîwhich performs best, and under what conditions?</strong></p>
  </li>
  <li>
    <p><strong>Is there a need to propose a new RDMA-based gRPC runtime</strong> that can <strong>adaptively select the best transport strategy</strong> for each workload?</p>
  </li>
  <li>
    <p><strong>If so, what is the measurable performance benefit</strong> gained through the proposed design (e.g., AR-gRPC) compared to traditional channels?</p>
  </li>
</ol>

<hr />

<h2 id="-overview-of-tensorflow-distributed-at-scale"><span class="me-2">üß† Overview of TensorFlow: Distributed at Scale</span><a href="#-overview-of-tensorflow-distributed-at-scale" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> is one of the most widely adopted open-source deep learning frameworks, developed by the Google Brain Team. It empowers users to build and train large-scale neural networks using a <strong>dataflow graph</strong> architecture, where:</p>

<ul>
  <li><strong>Nodes</strong> represent computation (mathematical operations),</li>
  <li><strong>Edges</strong> carry multidimensional data structures (tensors) between nodes.</li>
</ul>

<p>In a distributed setting, TensorFlow decomposes execution into four logical components:</p>

<ul>
  <li><strong>Client</strong> ‚Äì Defines the computational graph and initiates execution.</li>
  <li><strong>Master</strong> ‚Äì Receives the graph, optimizes it, and coordinates execution.</li>
  <li><strong>Workers</strong> ‚Äì Perform the actual computation (on CPU, GPU, or TPU).</li>
  <li><strong>Parameter Servers (PS)</strong> ‚Äì Handle model updates and state.</li>
</ul>

<p>The client constructs the graph and sends it to the master via a session using <strong>Protocol Buffers</strong>. The master prunes and partitions the graph, then dispatches it to workers and parameter servers. During training, workers compute gradients and send them to PS nodes, which update and serve back the latest model parameters.</p>

<p><a href="/blog/assets/img/tensor_grpc.png" class="popup img-link shimmer"><img src="/blog/assets/img/tensor_grpc.png" alt="Tensor Flow and gRPC" loading="lazy"></a>
<a href="/blog/assets/img/tensor_flow_workers.png" class="popup img-link shimmer"><img src="/blog/assets/img/tensor_flow_workers.png" alt="" loading="lazy"></a></p>

<h3 id="-communication-in-distributed-tensorflow"><span class="me-2">üì§ Communication in Distributed TensorFlow</span><a href="#-communication-in-distributed-tensorflow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>The most communication-intensive phase is <strong>tensor transmission</strong> between workers and PS nodes. TensorFlow supports several communication backends to handle this:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">gRPC</code> over TCP/IP (default)</li>
  <li><code class="language-plaintext highlighter-rouge">gRPC + MPI</code></li>
  <li><code class="language-plaintext highlighter-rouge">gRPC + Verbs</code></li>
  <li><code class="language-plaintext highlighter-rouge">AR-gRPC</code> (Adaptive RDMA-based gRPC, discussed in this blog)</li>
</ul>

<p>Each of these channels offers different performance trade-offs, particularly in how efficiently they move tensors across nodes.</p>

<hr />

<h2 id="-overview-of-grpc-the-backbone-of-tensorflow-communication"><span class="me-2">üì¨ Overview of gRPC: The Backbone of TensorFlow Communication</span><a href="#-overview-of-grpc-the-backbone-of-tensorflow-communication" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><a href="https://grpc.io/">gRPC</a> is TensorFlow‚Äôs <strong>core communication mechanism</strong>, especially for orchestrating administrative tasks and managing tensor transfers in distributed environments. Whether it‚Äôs simple TCP or advanced RDMA-based channels, gRPC underpins the framework‚Äôs communication stack.</p>

<p>Beyond TensorFlow, gRPC is widely adopted across the industry‚Äîused by companies like <strong>Netflix</strong>, <strong>Cisco</strong>, and <strong>Juniper</strong> for service-to-service communication.</p>

<p>Here‚Äôs how it works at a high level:</p>

<ul>
  <li>A client (e.g., Python) and a server (e.g., C++) communicate using <strong>Protocol Buffers</strong> (protobufs) for defining request and response messages.</li>
  <li>gRPC abstracts the network details using a component called an <strong>Endpoint</strong>.</li>
  <li>Each <strong>Endpoint</strong> encapsulates a bidirectional channel and implements <code class="language-plaintext highlighter-rouge">Read</code> and <code class="language-plaintext highlighter-rouge">Write</code> methods over a transport protocol (e.g., TCP, UDP).</li>
</ul>

<h3 id="-extending-grpc-with-rdma"><span class="me-2">üîå Extending gRPC with RDMA</span><a href="#-extending-grpc-with-rdma" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>To accelerate tensor transmission, researchers have begun extending gRPC‚Äôs core to support <strong>RDMA-based endpoints</strong>, replacing traditional TCP sockets. This allows for:</p>

<ul>
  <li><strong>Kernel-bypass communication</strong></li>
  <li><strong>Zero-copy transfers</strong></li>
  <li><strong>Sub-microsecond latencies</strong></li>
</ul>

<h2 id="-characterizing-distributed-tensorflow-performance"><span class="me-2">üìä Characterizing Distributed TensorFlow Performance</span><a href="#-characterizing-distributed-tensorflow-performance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>To understand the impact of communication channels on training performance, we perform a microbenchmark study using a distributed TensorFlow setup and analyze how different transport mechanisms affect throughput.</p>

<h3 id="-communication-channels"><span class="me-2">üîó Communication Channels</span><a href="#-communication-channels" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>In TensorFlow, multiple communication backends are available for tensor transmission between workers and parameter servers:</p>

<ul>
  <li><strong>gRPC (default):</strong> Operates over IP-over-InfiniBand (IPoIB), which still incurs TCP/IP stack overhead.</li>
  <li><strong>gRPC + Verbs / gRPC + MPI:</strong> Utilize <strong>native RDMA</strong> for direct memory-to-memory communication‚Äîeliminating kernel involvement and improving performance.</li>
</ul>

<hr />

<h3 id="-cluster-configuration-experiment"><span class="me-2">üñß Cluster Configuration Experiment</span><a href="#-cluster-configuration-experiment" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>A <strong>4-node distributed TensorFlow cluster</strong> in <strong>Parameter Server (PS) mode</strong>:</p>

<ul>
  <li>üß† <strong>1 node (CPU)</strong> ‚Äì Hosts the Parameter Server.</li>
  <li>üéÆ <strong>3 nodes (GPU)</strong> ‚Äì Act as worker nodes responsible for training.</li>
</ul>

<p>This setup mirrors a real-world training environment where compute-heavy operations are GPU-accelerated, while the PS node manages parameter updates on the CPU.</p>

<h3 id="-benchmarking-setup"><span class="me-2">üß™ Benchmarking Setup</span><a href="#-benchmarking-setup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>For the evaluation, we use the <strong>ResNet-50</strong> deep neural network from the <strong>TensorFlow CNN Benchmark Suite</strong>:</p>

<ul>
  <li>üì¶ <strong>Training Mode:</strong> Synchronous</li>
  <li>üñºÔ∏è <strong>Input:</strong> Synthetic image data (to isolate communication performance)</li>
  <li>üßÆ <strong>Batch Size:</strong> 32 images per GPU</li>
  <li>üìà <strong>Metric:</strong> Total number of images processed per second</li>
</ul>

<p>ResNet-50 is a moderately deep convolutional architecture‚Äîcomplex enough to stress both computation and communication pipelines‚Äîmaking it ideal for evaluating distributed training performance across different channels.</p>

<h3 id="-performance-comparison-throughput-by-channel"><span class="me-2">üìà Performance Comparison: Throughput by Channel</span><a href="#-performance-comparison-throughput-by-channel" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>Below is the observed training throughput (in images per second) for each communication backend during synchronous ResNet-50 training:</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th><strong>Channel</strong></th>
      <th><strong>Images/Sec</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gRPC</td>
      <td>91.06</td>
    </tr>
    <tr>
      <td>gRPC + Verbs</td>
      <td>103.21</td>
    </tr>
    <tr>
      <td>gRPC + MPI</td>
      <td>84.45</td>
    </tr>
  </tbody>
</table></div>

<h4 id="-observations"><span class="me-2">üîç Observations</span><a href="#-observations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<ul>
  <li>‚úÖ <code class="language-plaintext highlighter-rouge">gRPC + Verbs</code> shows a noticeable performance improvement over the default gRPC channel.</li>
  <li>‚ö†Ô∏è <code class="language-plaintext highlighter-rouge">gRPC + MPI</code> underperforms compared to even the default gRPC, suggesting inefficiencies in tensor transfer using MPI in this setup.</li>
</ul>

<h3 id="-grpc-bottleneck-chunking-overhead"><span class="me-2">üìâ gRPC Bottleneck: Chunking Overhead</span><a href="#-grpc-bottleneck-chunking-overhead" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Further profiling revealed a significant limitation in the default <strong>socket-based gRPC</strong> communication path:</p>

<blockquote>
  <p>We observed that communication over the gRPC channel involved a mix of <strong>many short messages</strong> and <strong>large messages (up to 4 MB)</strong>.<br />
The 4 MB threshold is due to <strong>gRPC‚Äôs default maximum payload limit</strong>.</p>
</blockquote>

<p>However, in real training scenarios like <strong>ResNet-50</strong>, the actual tensor payloads often <strong>exceed 4 MB</strong>. As a result, gRPC is forced to <strong>naively chunk large tensors</strong> into smaller segments‚Äîintroducing overhead, especially over TCP/IP on high-performance interconnects like InfiniBand.</p>

<p>This naive chunking strategy becomes a <strong>major communication bottleneck</strong>, limiting the overall throughput even when high-speed network hardware is available.</p>

<h2 id="-deep-dive-characterizing-tensorflow-communication-channels"><span class="me-2">üß™ Deep Dive: Characterizing TensorFlow Communication Channels</span><a href="#-deep-dive-characterizing-tensorflow-communication-channels" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>To understand how different communication backends behave during training, we profiled the <strong>payload patterns</strong>, <strong>flow mechanisms</strong>, and <strong>performance bottlenecks</strong> across three major TensorFlow channels: <strong>gRPC</strong>, <strong>gRPC + Verbs</strong>, and <strong>gRPC + MPI</strong>.</p>

<p><a href="/blog/assets/img/grpc_payload.png" class="popup img-link shimmer"><img src="/blog/assets/img/grpc_payload.png" alt="gRPC Payload" loading="lazy"></a>
<a href="/blog/assets/img/communication_flow.png" class="popup img-link shimmer"><img src="/blog/assets/img/communication_flow.png" alt="Communication Flow" loading="lazy"></a></p>

<p>These insights are drawn from an empirical study of ResNet-50 training on a distributed TensorFlow setup.</p>

<hr />

<h3 id="-grpc-channel-chunking-overhead-and-copying-costs"><span class="me-2">üîç gRPC Channel: Chunking Overhead and Copying Costs</span><a href="#-grpc-channel-chunking-overhead-and-copying-costs" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>We began by profiling <strong>payload sizes transmitted over the default gRPC channel</strong>. A sample of ~2K payload traces <strong>[Fig: Payload over gRPC]</strong> collected from a worker node revealed a mix of:</p>

<ul>
  <li>Numerous <strong>short messages</strong>, and</li>
  <li><strong>Large messages up to 4 MB</strong> ‚Äî the default gRPC max payload size.</li>
</ul>

<p>This 4 MB upper limit forces gRPC to <strong>naively chunk large tensors</strong> into smaller packets. Yet, ResNet-50 training often involves <strong>payloads larger than 4 MB</strong>, making this chunking strategy suboptimal on high-performance networks.</p>

<h4 id="-root-cause"><span class="me-2">üí° Root Cause:</span><a href="#-root-cause" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>gRPC relies on <strong><code class="language-plaintext highlighter-rouge">sendmsg</code>/<code class="language-plaintext highlighter-rouge">recvmsg</code></strong> for communication, which involves:</p>

<ul>
  <li>Allocating new memory for large payloads,</li>
  <li>Copying data into internal buffers (especially for messages &gt;2 KB),</li>
  <li>Using <code class="language-plaintext highlighter-rouge">iovec</code> structures to compose multi-buffer messages.</li>
</ul>

<p>While efficient for small messages, this design becomes a <strong>bottleneck for large-scale tensor transfers</strong>‚Äîdue to repeated copying and memory allocation overhead.</p>

<hr />

<h3 id="-grpc--verbs-fast-path-but-not-fully-optimized"><span class="me-2">üöÄ gRPC + Verbs: Fast Path, But Not Fully Optimized</span><a href="#-grpc--verbs-fast-path-but-not-fully-optimized" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>In this setup, <strong>gRPC handles control flow</strong>, while <strong>tensor payloads are transferred via RDMA Verbs</strong>. <strong>[Fig: Payload over gRPC + Payload over verbs]</strong> shows the payload distributions over gRPC+Verbs</p>

<p>The Verbs backend uses <strong>RDMA Write</strong> to push data into pre-pinned buffers on the receiver side. Profiling showed that payloads were mostly around <strong>512 Bytes</strong>, which‚Äîas previous RDMA studies show‚Äîis not optimal for throughput.</p>

<h4 id="Ô∏è-key-bottlenecks"><span class="me-2">‚ö†Ô∏è Key Bottlenecks:</span><a href="#Ô∏è-key-bottlenecks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<ul>
  <li>Only <strong>RDMA Write</strong> is used for transmission, regardless of payload size.</li>
  <li>When a tensor outgrows the preallocated buffer, a <strong>new buffer is created, pinned, and exchanged</strong>‚Äîintroducing latency.</li>
  <li>Each tensor transfer involves <strong>multiple RDMA writes</strong> (e.g., for ACKs and flow control), which adds overhead.</li>
</ul>

<p>This design results in <strong>extra memory operations</strong> and <strong>unnecessary RDMA message exchanges</strong>, impacting performance.</p>

<hr />

<h3 id="-grpc--mpi-thread-overheads-and-rank-setup-costs"><span class="me-2">üìâ gRPC + MPI: Thread Overheads and Rank Setup Costs</span><a href="#-grpc--mpi-thread-overheads-and-rank-setup-costs" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>In the <strong>gRPC + MPI</strong> channel, administrative messages still go through gRPC, while <strong>tensor data is transferred over MPI</strong>, which can use RDMA-capable interconnects. <strong>[Fig: Payload over gRPC + Payload over MPI]</strong> indicates a wide range of payloads starting from 128 Bytes to 10 MBytes over the MPI channel.</p>

<p>Payload traces revealed a <strong>wide range of sizes (128B to 10MB)</strong>, indicating more dynamic behavior. However, performance was lower than both gRPC and gRPC+Verbs due to several internal design inefficiencies.</p>

<h4 id="-identified-issues"><span class="me-2">üîç Identified Issues:</span><a href="#-identified-issues" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<ul>
  <li>A <strong>dedicated MPI thread</strong> handles communication, leading to <strong>locking, context-switching, and serialization overhead</strong>.</li>
  <li><strong>MPI Isend/MRecv</strong> is used for data transfers, while <strong>MPI Improbe + ANY_SOURCE</strong> introduces extra CPU overhead for wildcard matching.</li>
  <li>Additional gRPC messages are exchanged to <strong>set up MPI ranks and tasks</strong>, increasing control traffic.</li>
</ul>

<p>These factors collectively result in <strong>higher communication latency</strong> and <strong>reduced training throughput</strong>‚Äîas confirmed in the earlier performance table.</p>

<h2 id="-conclusion-limitations--the-case-for-ar-grpc"><span class="me-2">‚úÖ Conclusion: Limitations &amp; the Case for AR-gRPC</span><a href="#-conclusion-limitations--the-case-for-ar-grpc" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>From analysis and profiling of TensorFlow‚Äôs communication stack across different channels, several key insights emerge:</p>

<ol>
  <li>
    <p>üßµ <strong>DNN training workloads involve a wide spectrum of message sizes</strong> ‚Äî from small control messages to large 10MB+ tensor payloads (as seen in ResNet-50). Optimizing communication for large tensors is essential, especially as model complexity grows.</p>
  </li>
  <li>
    <p>‚ö†Ô∏è <strong>The current gRPC, gRPC+Verbs, and gRPC+MPI implementations in TensorFlow all exhibit limitations</strong> when leveraging RDMA-capable networks. Despite using high-performance interconnects, inefficiencies in memory management, protocol design, or threading prevent optimal performance.</p>
  </li>
  <li>üîÅ <strong>Even in RDMA-enabled channels (Verbs/MPI), some data still travels over the default TCP/IP-based gRPC</strong>, which negates some of the potential performance gains. Moreover, maintaining <strong>two separate runtimes</strong> (gRPC for control, MPI/Verbs for tensors) introduces:
    <ul>
      <li>Resource contention,</li>
      <li>Coordination overhead,</li>
      <li>Potential deadlocks or performance interference.</li>
    </ul>
  </li>
  <li>üîÑ <strong>None of these existing channels offer an adaptive communication mechanism</strong>‚Äîi.e., they do not adjust dynamically to tensor sizes or workload patterns. Deep learning workloads are inherently variable, and fixed communication strategies can‚Äôt fully exploit network and system capabilities.</li>
</ol>

<hr />

<h3 id="-the-challenges"><span class="me-2">üß† The Challenges</span><a href="#-the-challenges" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>This brings us to a fundamental question for the TensorFlow and systems community:</p>

<blockquote>
  <p><em>Can we design a <strong>unified</strong>, high-performance communication runtime for TensorFlow‚Äîone that supports adaptive, RDMA-accelerated data movement through a single gRPC stack?</em></p>
</blockquote>

<p>While there have been early attempts at RDMA integration in gRPC, existing implementations fall short due to several limitations:</p>

<ul>
  <li>No support for <strong>one-sided RDMA operations</strong></li>
  <li>Absence of <strong>adaptive strategies</strong> based on message characteristics</li>
  <li>Use of <strong>interrupt-based signaling</strong>, which increases latency</li>
</ul>

<hr />

<h3 id="-looking-ahead-ar-grpc"><span class="me-2">üåü Looking Ahead: AR-gRPC</span><a href="#-looking-ahead-ar-grpc" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>To address these challenges, Paper introduced <strong>AR-gRPC (Adaptive RDMA-based gRPC)</strong>‚Äîa redesigned, RDMA-optimized gRPC runtime tailored specifically for deep learning workloads. AR-gRPC:</p>

<ul>
  <li>Offers <strong>unified communication</strong> for both control and tensor transfer,</li>
  <li>Leverages <strong>adaptive switching</strong> between RDMA and socket paths,</li>
  <li>Supports <strong>zero-copy</strong>, one-sided RDMA operations for large tensors,</li>
  <li>Delivers <strong>lower latency</strong> and <strong>higher throughput</strong> across diverse training scenarios.</li>
</ul>

<h2 id="Ô∏è-design-of-ar-grpc-efficient-rdma-communication-for-tensorflow"><span class="me-2">üèóÔ∏è Design of AR-gRPC: Efficient RDMA Communication for TensorFlow</span><a href="#Ô∏è-design-of-ar-grpc-efficient-rdma-communication-for-tensorflow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>AR-gRPC introduces a high-performance RDMA-based communication runtime that operates over <strong>InfiniBand</strong> and <strong>RoCE</strong>, aiming to minimize latency and maximize throughput in distributed deep learning frameworks like TensorFlow.</p>

<p>At the core of this design is a novel abstraction called the <strong>RDMA-Endpoint</strong>, which integrates directly into the existing gRPC architecture while enabling <strong>zero-copy data transfer</strong> using RDMA.</p>

<h3 id="-rdma-endpoint-extending-grpc-for-high-speed-networks"><span class="me-2">üîå RDMA-Endpoint: Extending gRPC for High-Speed Networks</span><a href="#-rdma-endpoint-extending-grpc-for-high-speed-networks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>The <code class="language-plaintext highlighter-rouge">RDMA-Endpoint</code> extends the default <code class="language-plaintext highlighter-rouge">Endpoint</code> abstraction defined in gRPC to support RDMA transports. It encapsulates a connection between an RDMA-capable client and server and exposes three key operations:</p>

<ul>
  <li><strong>RDMA-Endpoint-Write</strong></li>
  <li><strong>RDMA-Endpoint-Read</strong></li>
  <li><strong>RDMA-Polling</strong></li>
</ul>

<p>This modular design allows RDMA-based communication to be integrated into existing systems without altering the upper application layers.</p>

<p><a href="/blog/assets/img/grpc_and_communication.png" class="popup img-link shimmer"><img src="/blog/assets/img/grpc_and_communication.png" alt="Communication Flew" loading="lazy"></a></p>

<h3 id="Ô∏è-rdma-endpoint-write"><span class="me-2">‚úâÔ∏è RDMA-Endpoint-Write</span><a href="#Ô∏è-rdma-endpoint-write" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Serialized messages (e.g., Protocol Buffers) generated by the application layer are transmitted using <code class="language-plaintext highlighter-rouge">RDMA-Endpoint-Write</code>. This component writes the payload directly to a <strong>pinned RDMA buffer</strong>, allowing the system to bypass the kernel and avoid redundant data copies.</p>

<p>By relying on <strong>one-sided RDMA Write operations</strong>, the implementation significantly reduces communication overhead and improves performance for large message transfers, such as tensors.</p>

<h3 id="-rdma-polling-completion-detection-strategy"><span class="me-2">üîÑ RDMA-Polling: Completion Detection Strategy</span><a href="#-rdma-polling-completion-detection-strategy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>To efficiently detect send and receive completions, AR-gRPC utilizes <strong>busy polling</strong> on RDMA Completion Queues. Modern multi-core processors enable the allocation of dedicated cores to handle polling operations without impacting application threads.</p>

<p>This polling thread repeatedly checks for message completion events, ensuring:</p>
<ul>
  <li>Consistent low-latency communication,</li>
  <li>High throughput for small and large payloads,</li>
  <li>Efficient core utilization in high-performance cluster environments.</li>
</ul>

<h3 id="-rdma-endpoint-read"><span class="me-2">üì• RDMA-Endpoint-Read</span><a href="#-rdma-endpoint-read" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Incoming RDMA messages are received by the polling thread and forwarded to <code class="language-plaintext highlighter-rouge">RDMA-Endpoint-Read</code>. This handler:</p>

<ol>
  <li>Constructs application-level payloads from data in the RDMA buffer,</li>
  <li>Passes the deserialized payload to the upper-layer runtime for processing.</li>
</ol>

<p>This read path ensures that received messages are quickly and efficiently delivered to the application with minimal memory overhead.</p>

<h2 id="-adaptive-communication-strategy-in-ar-grpc"><span class="me-2">üîÑ Adaptive Communication Strategy in AR-gRPC</span><a href="#-adaptive-communication-strategy-in-ar-grpc" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Existing communication designs in TensorFlow‚Äîwhether based on gRPC, MPI, or Verbs‚Äîsuffer from rigid protocol strategies that fail to efficiently adapt to the diverse tensor sizes found in deep learning workloads. AR-gRPC introduces a <strong>hybrid, adaptive communication mechanism</strong> that chooses the most efficient RDMA operation based on the message size and system characteristics.</p>

<hr />

<h3 id="Ô∏è-hybrid-rdma-protocol-eager-vs-rendezvous"><span class="me-2">‚öôÔ∏è Hybrid RDMA Protocol: Eager vs Rendezvous</span><a href="#Ô∏è-hybrid-rdma-protocol-eager-vs-rendezvous" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>AR-gRPC combines the strengths of <strong>eager</strong> and <strong>rendezvous</strong> protocols to optimize performance across all message sizes:</p>

<ul>
  <li><strong>üîπ Eager Protocol (for small messages):</strong>
    <ul>
      <li>Utilizes <strong>two-sided RDMA Send/Recv</strong>.</li>
      <li>Ideal for low-latency delivery of control messages and small tensors.</li>
      <li>The <strong>eager threshold</strong> (message size cutoff) is <strong>auto-tuned</strong> based on the network architecture and observed throughput.</li>
    </ul>
  </li>
  <li><strong>üî∏ Rendezvous Protocol (for large messages):</strong>
    <ul>
      <li>Employs <strong>one-sided RDMA Read</strong> from the receiver‚Äôs side.</li>
      <li>Reduces control message overhead by requiring only a <strong>Request-To-Send (RTS)</strong> from sender before the receiver reads the data.</li>
    </ul>
  </li>
</ul>

<p>This adaptive design outperforms the fixed strategy used in TensorFlow‚Äôs Verbs-based channel, which relies solely on <strong>RDMA Write</strong> with multiple control messages like RTS and CTS (Clear-To-Send) for each transfer.</p>

<hr />

<h3 id="-efficiency-gains-through-design-differences"><span class="me-2">üìà Efficiency Gains Through Design Differences</span><a href="#-efficiency-gains-through-design-differences" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>The AR-gRPC strategy delivers multiple advantages over traditional implementations:</p>

<ul>
  <li>
    <p><strong>Protocol Selection:</strong><br />
Automatically chooses between RDMA Send and RDMA Read based on runtime message size, eliminating the one-size-fits-all bottleneck seen in static designs.</p>
  </li>
  <li>
    <p><strong>Reduced Control Traffic:</strong><br />
RDMA Read requires fewer round trips than RDMA Write with CTS-style protocols, lowering latency and CPU involvement.</p>
  </li>
  <li>
    <p><strong>Decoupled Buffer Management:</strong><br />
The buffer management layer is <strong>independent of the RDMA protocol logic</strong>, enabling greater flexibility in memory reuse, pinning, and flow control.<br />
In contrast, the Verbs-based TensorFlow channel tightly couples message and acknowledgment buffers per RDMA connection, limiting scalability.</p>
  </li>
</ul>

<blockquote>
  <p>üß† Adaptive RDMA-based communication in AR-gRPC delivers lower latency, smarter protocol selection, and better utilization of system resources‚Äîespecially in environments with varied message sizes and high-speed interconnects.</p>
</blockquote>

<h2 id="-message-pipelining-coalescing-and-zero-copy-in-ar-grpc"><span class="me-2">üßµ Message Pipelining, Coalescing, and Zero-Copy in AR-gRPC</span><a href="#-message-pipelining-coalescing-and-zero-copy-in-ar-grpc" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>In distributed deep learning, communication workloads exhibit a wide range of message sizes‚Äîfrom small control updates to large tensor payloads. Optimizing these transfers is critical for minimizing training time and maximizing resource efficiency. AR-gRPC introduces several enhancements to gRPC‚Äôs data path to address performance limitations related to buffering, latency, and memory copying.</p>

<hr />

<h3 id="-efficient-pipelining-for-large-messages"><span class="me-2">üì¶ Efficient Pipelining for Large Messages</span><a href="#-efficient-pipelining-for-large-messages" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TensorFlow and gRPC use <code class="language-plaintext highlighter-rouge">iovec</code> structures for managing message buffers. These buffers are often <strong>asymmetric in size</strong>, especially when mixed control and tensor data are involved. A naive implementation might:</p>

<ul>
  <li>Copy all <code class="language-plaintext highlighter-rouge">iovec</code> segments into a pinned RDMA buffer, and</li>
  <li>Issue a <strong>blocking RDMA send</strong>, waiting for completion.</li>
</ul>

<p>While functional, this approach <strong>scales poorly for large tensors</strong>, as it stalls the sending thread during transfer. To address this, AR-gRPC implements:</p>

<ul>
  <li><strong>Chunking</strong> of large payloads based on a configurable threshold.</li>
  <li><strong>Auto-tuning</strong> of chunk size based on architectural characteristics (e.g., network MTU, memory bandwidth).</li>
  <li><strong>Asynchronous dispatch</strong> of chunked sends to avoid thread blocking.</li>
</ul>

<hr />

<h3 id="-concurrent-receiving-with-rdma-endpoint-read"><span class="me-2">‚ö° Concurrent Receiving with RDMA-Endpoint-Read</span><a href="#-concurrent-receiving-with-rdma-endpoint-read" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>On the receiver side, a major design decision involves when to invoke the RDMA read handler:</p>

<ul>
  <li><strong>Waiting for all chunks</strong> introduces unnecessary delay.</li>
  <li><strong>Triggering per chunk</strong>, as done in AR-gRPC, allows <strong>early and parallel processing</strong>.</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">RDMA-Endpoint-Read</code> in AR-gRPC is designed to <strong>invoke receiving callbacks immediately upon the arrival of each chunk</strong>. This enables:</p>

<ul>
  <li><strong>High concurrency</strong> in message reconstruction,</li>
  <li>Faster streaming of large tensors,</li>
  <li>Better pipeline utilization for workloads with bursty traffic patterns.</li>
</ul>

<hr />

<h3 id="-coalescing-small-iovec-buffers"><span class="me-2">üß© Coalescing Small Iovec Buffers</span><a href="#-coalescing-small-iovec-buffers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>For messages composed of many small <code class="language-plaintext highlighter-rouge">iovec</code> buffers‚Äîcommon in control flows‚Äîsending each buffer individually is inefficient. AR-gRPC instead:</p>

<ul>
  <li><strong>Coalesces multiple small iovec segments</strong> (up to the eager send threshold) into a single pinned RDMA buffer.</li>
  <li><strong>Maintains the original ordering</strong> of buffers.</li>
  <li>Sends the coalesced payload using <strong>eager RDMA send protocol</strong>.</li>
</ul>

<p>This minimizes the number of RDMA operations, reduces control message overhead, and improves bandwidth utilization, especially in high message rate scenarios.</p>

<hr />

<h3 id="-zero-copy-transmission"><span class="me-2">üîÑ Zero-Copy Transmission</span><a href="#-zero-copy-transmission" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>One hidden inefficiency in standard gRPC is an extra memory copy between TensorFlow tensors and gRPC message buffers. This occurs because of intermediate serialization layers.</p>

<p>To enable <strong>true zero-copy</strong>, AR-gRPC takes advantage of TensorFlow‚Äôs memory allocation behavior:</p>

<ul>
  <li>When TensorFlow allocates a gRPC byte buffer to send a tensor, AR-gRPC <strong>overrides this allocation</strong> to use an <strong>RDMA-pinned buffer</strong> from its internal pool.</li>
  <li>No code changes are required in TensorFlow itself‚Äîonly a minor modification in the gRPC layer ensures that pinned buffers are transparently used.</li>
</ul>

<p>This approach eliminates redundant memory copies and allows <strong>direct memory access from RDMA NIC to TensorFlow‚Äôs tensor memory</strong>, significantly reducing CPU and memory overhead.</p>

<blockquote>
  <p>üß† By combining pipelining, chunk-aware read callbacks, buffer coalescing, and zero-copy transfers, AR-gRPC delivers a communication pipeline that is both lightweight and highly scalable‚Äîideal for training modern deep neural networks at scale.</p>
</blockquote>

<h2 id="-conclusion"><span class="me-2">‚úÖ Conclusion</span><a href="#-conclusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>As deep learning workloads continue to scale across larger models and distributed GPU clusters, the need for efficient, scalable communication primitives becomes increasingly critical. Traditional gRPC channels in TensorFlow‚Äîwhether over TCP/IP, Verbs, or MPI‚Äîhave shown performance bottlenecks due to rigid protocol choices, inefficient handling of varying tensor sizes, and non-optimal use of RDMA hardware capabilities.</p>

<p><strong>AR-gRPC</strong> reimagines this layer with the following key innovations:</p>

<ul>
  <li>üîÑ <strong>Adaptive Protocol Switching</strong> between eager (RDMA Send/Recv) and rendezvous (RDMA Read) strategies.</li>
  <li>üßµ <strong>Message Pipelining and Coalescing</strong> to maximize throughput and concurrency.</li>
  <li>üß† <strong>Receiver-side Chunk Processing</strong> via RDMA-Endpoint-Read for low-latency response.</li>
  <li>üö´ <strong>Zero-Copy Tensor Transmission</strong> to eliminate redundant memory operations.</li>
</ul>

<p>These enhancements result in significantly improved performance and scalability for distributed TensorFlow workloads‚Äîespecially for deep networks like ResNet-50, which involve both small control messages and large tensor transfers.</p>

<blockquote>
  <p>üìå The design and insights presented here are inspired by the academic work <a href="https://par.nsf.gov/servlets/purl/10112767">Characterizing and Optimizing Distributed Deep Learning Training on Modern GPU Clusters</a>, and extend it with practical implementation strategies and system-level engineering improvements.</p>
</blockquote>

<p>By embracing an <strong>architecture-aware</strong>, <strong>message-size-adaptive</strong>, and <strong>zero-copy</strong> approach, AR-gRPC demonstrates how communication runtimes can evolve to meet the demands of modern AI workloads.</p>

<hr />

<h3 id="Ô∏è-future-directions"><span class="me-2">üõ†Ô∏è Future Directions</span><a href="#Ô∏è-future-directions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Looking ahead, further research and development can explore:</p>

<ul>
  <li>Integrating AR-gRPC natively into newer TensorFlow runtime versions.</li>
  <li>Extending support to frameworks like PyTorch and JAX.</li>
  <li>Dynamic congestion-aware protocol switching based on real-time network telemetry.</li>
</ul>

<p>As deep learning infrastructure grows in complexity, the boundary between model performance and system-level communication becomes ever more critical. Solutions like AR-gRPC help bridge that gap‚Äîpaving the way for faster, smarter, and more efficient distributed AI systems.</p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/blog/categories/hpc/">hpc</a>,
          <a href="/blog/categories/rdma/">rdma</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/blog/tags/rdma/"
            class="post-tag no-text-decoration"
          >rdma</a>
        
          <a
            href="/blog/tags/tensorflow/"
            class="post-tag no-text-decoration"
          >tensorflow</a>
        
          <a
            href="/blog/tags/grpc/"
            class="post-tag no-text-decoration"
          >gRPC</a>
        
          <a
            href="/blog/tags/mpi/"
            class="post-tag no-text-decoration"
          >mpi</a>
        
          <a
            href="/blog/tags/hpc/"
            class="post-tag no-text-decoration"
          >hpc</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=%F0%9F%9A%80%20Accelerating%20TensorFlow%20with%20Adaptive%20RDMA%20%E2%9A%A1%EF%B8%8F:%20Supercharging%20gRPC%20Performance%20%F0%9F%94%97%20-%20SayantanKhan&url=http%3A%2F%2F0.0.0.0%3A4000%2Fblog%2Fposts%2Ftensorflow_grpc%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=%F0%9F%9A%80%20Accelerating%20TensorFlow%20with%20Adaptive%20RDMA%20%E2%9A%A1%EF%B8%8F:%20Supercharging%20gRPC%20Performance%20%F0%9F%94%97%20-%20SayantanKhan&u=http%3A%2F%2F0.0.0.0%3A4000%2Fblog%2Fposts%2Ftensorflow_grpc%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2F0.0.0.0%3A4000%2Fblog%2Fposts%2Ftensorflow_grpc%2F&text=%F0%9F%9A%80%20Accelerating%20TensorFlow%20with%20Adaptive%20RDMA%20%E2%9A%A1%EF%B8%8F:%20Supercharging%20gRPC%20Performance%20%F0%9F%94%97%20-%20SayantanKhan" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/backward_propagation/">Backpropagation, Softmax, Cross-Entropy, and the Chain Rule ‚Äî Explained with Equations</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/tensorflow_grpc/">üöÄ Accelerating TensorFlow with Adaptive RDMA ‚ö°Ô∏è: Supercharging gRPC Performance üîó</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/firecracker-vm/">üî• Firecracker: The Lightweight Guard of Serverless Computing</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/flexi-raft/">FlexiRaft: Flexible Quorums with Raft in MySQL</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/blog/posts/ha-cluster/">High Availability Demo üöÄ Pacemaker‚ÄØ+‚ÄØCorosync‚ÄØ+‚ÄØDocker</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/aws/">aws</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/backpropagation/">backpropagation</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/clustering/">clustering</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/consensus/">consensus</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/corosync/">corosync</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/cross-entropy/">cross-entropy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/distributed-systems/">distributed-systems</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/docker/">docker</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/flexiquorum/">flexiquorum</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/blog/posts/firecracker-vm/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>üî• Firecracker: The Lightweight Guard of Serverless Computing</p>
    </a>
  

  
    <a
      href="/blog/posts/backward_propagation/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>Backpropagation, Softmax, Cross-Entropy, and the Chain Rule ‚Äî Explained with Equations</p>
    </a>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>¬©
    <time>2025</time>

    
      <a href="https://www.linkedin.com/in/sayantan-khan-219231b7">Sayantan</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.3.0"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/aws/">aws</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/backpropagation/">backpropagation</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/clustering/">clustering</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/consensus/">consensus</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/corosync/">corosync</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/cross-entropy/">cross-entropy</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/distributed-systems/">distributed-systems</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/docker/">docker</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/blog/tags/flexiquorum/">flexiquorum</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/blog/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

